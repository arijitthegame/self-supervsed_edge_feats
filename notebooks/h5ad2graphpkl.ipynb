{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext memory_profiler\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sc.settings.verbosity=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load AnnData obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded @200604.10:56:07\n",
      "took 11.03-s to load data\n",
      "peak memory: 31671.86 MiB, increment: 11224.64 MiB\n"
     ]
    }
   ],
   "source": [
    "pdfp = '/home/ngr4/project/scni/data/processed_200108/'\n",
    "pfp = '/home/ngr4/project/scni/results/'\n",
    "\n",
    "def loader(fname,fpath,backed=None) : \n",
    "    start = time.time()\n",
    "    adata = sc.read_h5ad(filename=os.path.join(fpath,fname),backed=backed)\n",
    "    print('loaded @'+datetime.datetime.now().strftime('%y%m%d.%H:%M:%S'))\n",
    "    print('took {:.2f}-s to load data'.format(time.time()-start))\n",
    "    return adata\n",
    "\n",
    "def writer(fname,fpath,AnnData) :\n",
    "    start = time.time()\n",
    "    AnnData.write(os.path.join(fpath,fname))\n",
    "    print('saved @'+datetime.datetime.now().strftime('%y%m%d.%H:%M:%S'))\n",
    "    print('took {:.2f}-s to save data'.format(time.time()-start))\n",
    "    \n",
    "\n",
    "if True :\n",
    "    # load personal\n",
    "    fname='adata_transduction.h5ad'\n",
    "    %memit adata = loader(fname,pdfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Induction\n",
    "\n",
    "Take 40p of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing PCA\n",
      "    with n_comps=100\n",
      "    finished (0:05:08)\n",
      "computing neighbors\n",
      "    using 'X_pca' with n_pcs = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/ycga/project/dijk/ngr4/conda_envs/rnavel/lib/python3.7/site-packages/numba/typed_passes.py:271: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/rnavel/lib/python3.7/site-packages/umap/rp_tree.py\", line 135:\n",
      "@numba.njit(fastmath=True, nogil=True, parallel=True)\n",
      "def euclidean_random_projection_split(data, indices, rng_state):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/gpfs/ycga/project/dijk/ngr4/conda_envs/rnavel/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/rnavel/lib/python3.7/site-packages/umap/utils.py\", line 409:\n",
      "@numba.njit(parallel=True)\n",
      "def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):\n",
      "^\n",
      "\n",
      "  current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n",
      "/gpfs/ycga/project/dijk/ngr4/conda_envs/rnavel/lib/python3.7/site-packages/numba/typed_passes.py:271: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/rnavel/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    finished (0:01:17)\n",
      "computing PCA\n",
      "    with n_comps=100\n",
      "    finished (0:02:11)\n",
      "computing neighbors\n",
      "    using 'X_pca' with n_pcs = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/ycga/project/dijk/ngr4/conda_envs/rnavel/lib/python3.7/site-packages/numba/typed_passes.py:271: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/rnavel/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    finished (0:00:20)\n",
      "computing PCA\n",
      "    with n_comps=100\n",
      "    finished (0:01:16)\n",
      "computing neighbors\n",
      "    using 'X_pca' with n_pcs = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/ycga/project/dijk/ngr4/conda_envs/rnavel/lib/python3.7/site-packages/numba/typed_passes.py:271: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/rnavel/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    finished (0:00:11)\n"
     ]
    }
   ],
   "source": [
    "# split the data AND kick out mock infected cells as these are noise\n",
    "idx_train, idx_nottrain = train_test_split(adata.obs.index, train_size=0.4)\n",
    "tdata = sc.AnnData(X=adata[(adata.obs.index.isin(idx_train)),:].X,\n",
    "                          obs=adata[(adata.obs.index.isin(idx_train)),:].obs)\n",
    "idx_val, idx_test = train_test_split(idx_nottrain, train_size=0.25)\n",
    "idx_test, _ = train_test_split(idx_test, train_size=0.20)\n",
    "val = sc.AnnData(X=adata[(adata.obs.index.isin(idx_val)),:].X, \n",
    "                  obs=adata[(adata.obs.index.isin(idx_val)),:].obs)\n",
    "test = sc.AnnData(X=adata[(adata.obs.index.isin(idx_test)),:].X, \n",
    "                  obs=adata[(adata.obs.index.isin(idx_test)),:].obs)\n",
    "\n",
    "def graph_pp(AnnData, bbknn=False):\n",
    "    sc.tl.pca(AnnData, n_comps=100)\n",
    "    if bbknn:\n",
    "        sc.external.pp.bbknn(AnnData) # use default params\n",
    "    else:\n",
    "        sc.pp.neighbors(AnnData, n_pcs=100, n_neighbors=30)\n",
    "    return AnnData\n",
    "\n",
    "# make graph\n",
    "tdata = graph_pp(tdata, bbknn=False)\n",
    "val = graph_pp(val, bbknn=False)\n",
    "test = graph_pp(test, bbknn=False)\n",
    "\n",
    "if True:\n",
    "    feature_names = adata.var_names.to_list()\n",
    "    del adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding \n",
    "\n",
    "Select tasks for prediction\n",
    "\n",
    "1. yctype \n",
    "2. severe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode main label\n",
    "# ref: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE145926\n",
    "batch_encoder = {v:i for i,v in enumerate(tdata.obs['batch'].unique())}\n",
    "\n",
    "tdata.obs['ybatch'] = tdata.obs['batch'].map(batch_encoder)\n",
    "val.obs['ybatch'] = val.obs['batch'].map(batch_encoder)\n",
    "test.obs['ybatch'] = test.obs['batch'].map(batch_encoder)\n",
    "\n",
    "tdata.obs['yms'] = tdata.obs['MS'].astype(int)\n",
    "val.obs['yms'] = val.obs['MS'].astype(int)\n",
    "test.obs['yms'] = test.obs['MS'].astype(int)\n",
    "\n",
    "# encode ctype\n",
    "ctype_encoder = {v:i for i,v in enumerate(tdata.obs['louvain'].unique())}\n",
    "tdata.obs['yctype'] = tdata.obs['louvain'].map(ctype_encoder)\n",
    "val.obs['yctype'] = val.obs['louvain'].map(ctype_encoder)\n",
    "test.obs['yctype'] = test.obs['louvain'].map(ctype_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/ycga/project/dijk/ngr4/conda_envs/rnavel/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: This location for 'connectivities' is deprecated. It has been moved to .obsp[connectivities], and will not be accesible here in a future version of anndata.\n"
     ]
    }
   ],
   "source": [
    "# create dictionary\n",
    "def dictthat(AnnData, feat_names=None, gene_ranger=True):\n",
    "    \"\"\"Prep dictionary for export.\n",
    "    \n",
    "    If gene_ranger, divide by zero can occur for \n",
    "    non-expressing genes. Thus, will floor those\n",
    "    to 0.\n",
    "    \n",
    "    NOTE: customization re:y to predict is highly\n",
    "    dependent on user input. ERGO, modify this \n",
    "    \n",
    "    Arguments:\n",
    "        AnnData (sc.AnnData): with graph stuff\n",
    "        \n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    if gene_ranger:\n",
    "        # each gene in [0,1], divide by zeros to 0\n",
    "        minimum = AnnData.X.min(axis=0)\n",
    "        maximum = AnnData.X.max(axis=0)\n",
    "        if isinstance(minimum, np.ndarray):\n",
    "            num = AnnData.X - minimum\n",
    "        else:\n",
    "            num = AnnData.X - minimum.todense()\n",
    "        if isinstance((maximum - minimum), np.ndarray):\n",
    "            denom = (maximum - minimum)\n",
    "        else:\n",
    "            denom =  (maximum - minimum).todense()\n",
    "        xhat = np.divide(num, denom, out=np.zeros_like(num), where=denom!=0) \n",
    "    else:\n",
    "        # matrix in [0,1]\n",
    "        xhat = (AnnData.X - AnnData.X.min()) / (AnnData.X.max() - AnnData.X.min())\n",
    "        \n",
    "    \n",
    "\n",
    "    gdata = {'X':xhat,\n",
    "             'adj':AnnData.uns['neighbors']['connectivities']+sparse.diags([1]*AnnData.shape[0], format='csr')}\n",
    "    if feat_names is None:\n",
    "        gdata['feature_names'] = AnnData.var_names.to_list()\n",
    "    else:\n",
    "        gdata['feature_names'] = feat_names\n",
    "    gdata['cell_id'] = AnnData.obs.index.to_list()\n",
    "    for col in AnnData.obs.columns:\n",
    "        gdata[col] = AnnData.obs[col].to_list()\n",
    "    \n",
    "    return gdata\n",
    "\n",
    "gdata_train = dictthat(tdata, feat_names=feature_names)\n",
    "gdata_val = dictthat(val, feat_names=feature_names)\n",
    "gdata_test  = dictthat(test, feat_names=feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pklthat(gdata, fname, fpath=pdfp): \n",
    "    with open(os.path.join(fpath,fname),'wb') as f :\n",
    "        pickle.dump(gdata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "\n",
    "pklthat(gdata_train, 'scni_train_knn200604.pkl')\n",
    "pklthat(gdata_val, 'scni_val_knn200604.pkl')\n",
    "pklthat(gdata_test, 'scni_test_knn200604.pkl')\n",
    "\n",
    "# clean\n",
    "if True:\n",
    "    del tdata, test, gdata_train, gdata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ngr4/project/scni/data/processed_200108/'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
